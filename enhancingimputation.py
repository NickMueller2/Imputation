# -*- coding: utf-8 -*-
"""EnhancingImputation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1saTZWwIR5lGswyCYiaNXxPFuKJfihyJ1

### LIBRARIES
"""

!pip install fancyimpute

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
from pandas.api.types import is_string_dtype, is_bool_dtype, is_datetime64_dtype
import pandas.api.types as ptypes
import numpy as np
import csv
import time

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from matplotlib import pyplot
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report
# from fancyimpute import IterativeImputer

"""### FUNCTIONS"""

# Function to parse and categorize time of day
def categorize_time_of_day(mil_time):

    # Extract the hour from military time formatted as 'HH:MM:SS'
    hour = int(mil_time.split(':')[0])

    if 0 <= hour < 6:
        return 'Early Morning'
    elif 6 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 14:
        return 'Noon'
    elif 14 <= hour < 18:
        return 'Afternoon'
    elif 18 <= hour < 22:
        return 'Evening'
    else:
        return 'Night'

# Function to convert FBI codes
def convert_FBI_codes(code):
    if 'A' in code:
        code = code.replace('A', '.1')
    elif 'B' in code:
        code = code.replace('B', '.2')
    return float(code) if any(char.isdigit() for char in code) else code

# Function to convert IUCR codes
def convert_IUCR_codes(code):
    if code[-1].isalpha():  # If the last character is a letter
        letter = code[-1]
        number_part = code[:-1]
        new_code = number_part + letter_to_decimal[letter]
        return float(new_code) if number_part.lstrip('0') else new_code  # Convert to float, keep as str if leading zeros
    return float(code) if code.lstrip('0') else code  # Convert to float, keep as str if leading zeros


# Function to Plot Feature Importance
def PlotFeatureImportance(model, features, title='Variable Importances'):
  plt.figure(figsize=(18,6))

  # Create a DataFrame for better visualization
  importance = pd.DataFrame({
      'Feature': features,
      'Importance': model.feature_importances_
  })

  # Sort the DataFrame by importance
  importance = importance.sort_values(by='Importance', ascending=False, ignore_index=True)[0:100]

  # list of x locations for plotting
  x_values = importance['Feature']

  # Make a bar chart
  plt.bar(importance['Feature'], importance['Importance'], orientation = 'vertical')

  # Tick labels for x axis
  plt.xticks(x_values, importance['Feature'], rotation='vertical')

  # Axis labels and title
  plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title(title);

  return importance


def GetDataInfo(df, infer=False):
    print (f'Data Frame Columns = {df.shape[1]}\nData Frame Rows = {df.shape[0]}\n')
    if infer:
        df = df.infer_objects()

    var_info = df.dtypes.reset_index()
    var_info.columns = ['Column Name','DataType']

    # Compute additional statistics
    var_info['Uniques'] = df.nunique().values
    var_info['Missing'] = df.isnull().sum().values
    var_info['Missing %'] = (df.isnull().sum() / len(df) * 100).values
    var_info['Duplicates'] = df.apply(lambda col: col.duplicated().sum()).values

    var_info = var_info.set_index(var_info.columns[0]) # Make Column Name the index

    cat_list = df.columns[df.dtypes.map(lambda dtype: ptypes.is_object_dtype(dtype) or ptypes.is_categorical_dtype(dtype))]
    num_list =  df.columns[df.dtypes.map(ptypes.is_numeric_dtype)]
    bool_list = df.columns[df.dtypes.map(pd.api.types.is_bool_dtype)]
    dt_list = df.columns[df.dtypes.map(pd.api.types.is_datetime64_any_dtype)]

    return var_info, cat_list, num_list, bool_list, dt_list

from google.colab import drive
drive.mount('/content/drive')

"""### READ DATA"""

#from google.colab import drive
#drive.mount('/content/drive')

# Nicks
path = '/content/drive/MyDrive/Colab Notebooks/Crimes_-_2001_to_Present.csv'

# Jae
#path = '/content/drive/MyDrive/Google Colab/Crimes_-_2001_to_Present.csv'

# Steve
path = 'Crimes_-_2001_to_Present.csv'

df = pd.read_csv(path)
print(df.shape)
df.head()

# df.to_csv('crimes_1p5m.csv', encoding='utf-8', index=False)

"""### Subset Data for Faster Processing & Less Memory Errors"""

# Sample a smaller subset of the data for faster computation and reduced memory usage
df = df.sample(n=10000, random_state=42, ignore_index=True)  # Adjust 'n' as needed to fit memory constraints
df.head()

"""### DATA PREPROCESSING"""

df.dtypes

df['Arrest'].value_counts(dropna=False)

# Convert boolean 'Arrest' to 0 and 1
df['Arrest'] = pd.to_numeric(df['Arrest'], errors='coerce')

# Now, 'Arrest' column has 0s and 1s
# Convert boolean 'Domestic' to 0 and 1
df['Domestic'] = pd.to_numeric(df['Domestic'], errors='coerce')

# Now, 'Domestic' column has 0s and 1s

df['Beat']= df['Beat'].astype(str)
df['District']= df['District'].astype(str)
df['Ward']= df['Ward'].astype(str)
df['Community Area']= df['Community Area'].astype(str)
df['FBI Code']= df['FBI Code'].astype(str)
df.replace('nan', np.nan , inplace=True)

# Convert the 'Date' column to datetime format, specifying the format explicitly
df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %I:%M:%S %p')

# Extract day, month, year, and time (with AM/PM)
df['Day'] = df['Date'].dt.day
df['Month'] = df['Date'].dt.month
df['Year'] = df['Date'].dt.year
df['Time'] = df['Date'].dt.strftime('%I:%M:%S %p')  # Format time with AM/PM

# Convert 'Time' column to datetime format to ensure it's recognized as a time
df['Time'] = pd.to_datetime(df['Time'], format='%I:%M:%S %p')

# Format 'Time' column to military time (24-hour format)
df['Military_Time'] = df['Time'].dt.strftime('%H:%M:%S')

df['Military_Time'].dtype

# Apply the function to create a new column
df['time_of_day'] = df['Military_Time'].apply(categorize_time_of_day)

df = df.drop('Time', axis=1) # Remove the 'Time' column from the DataFrame
df = df.drop('Location', axis=1) # >800k unique
df = df.drop('Updated On', axis=1) # no value
df = df.drop('Case Number', axis=1) # id
df = df.drop('Date', axis=1) # feature engineered
df = df.drop('ID', axis=1) #id
df = df.drop('Military_Time', axis=1) # feature engineered

df.head()

df.describe()

# Prints the number of rows and columns
print(df.shape)
print(df.dtypes)

df.Arrest.value_counts(dropna=False)

df.describe(include=["O"])

# Assuming df is your existing DataFrame with the 'FBI code' column
# First, ensure all data in the column is in string format to handle conversions uniformly
df['FBI Code'] = df['FBI Code'].astype(str)

# Apply the function to the 'FBI code' column
# df['FBI Code'] = df['FBI Code'].apply(convert_FBI_codes)

# This will process the entire column converting codes like '08A' to '08.1', '08B' to '08.2', and convert numeric values as floats.

# Assuming df is your existing DataFrame with the 'IUCR' column
df['IUCR'] = df['IUCR'].astype(str)

# Dictionary to map letters to decimals
letter_to_decimal = {chr(i): f'.{i - 64}' for i in range(65, 91)}

# Optionally, keep original formatted strings with leading zeros for display/reference
df['IUCR'] = df['IUCR'].apply(lambda x: x[:-1] + letter_to_decimal[x[-1]] if x[-1].isalpha() else x)

# This will create two columns: 'IUCR_numeric' for calculations and 'IUCR_display' for display.

# Convert Boolean to numeric
df['Arrest'] = df['Arrest'].astype('int')
df['Domestic'] = df['Domestic'].astype('int')

df.dtypes

"""### Check Correlation"""

nums = ['Arrest','Domestic', 'X Coordinate', 'Y Coordinate', 'Year', 'Latitude','Longitude','Day','Month']

corr_matrix = df[nums].corr(method = "spearman").abs()

ax = plt.axes()
sns.heatmap(corr_matrix, cmap="YlGnBu", ax = ax)
ax.set_title('Correlation Heatmap')
sns.set(rc={'figure.figsize':(6,4)}) # increase seaborn plot size
plt.show()

df.drop('X Coordinate', axis=1, inplace=True) # correlation to Longitude
df.drop('Y Coordinate', axis=1, inplace=True) # correlation to Latitude

"""### Recheck Correlation"""

nums = ['Arrest','Domestic', 'Year', 'Latitude','Longitude','Day','Month']

corr_matrix = df[nums].corr(method = "spearman").abs()

ax = plt.axes()
sns.heatmap(corr_matrix, cmap="YlGnBu", ax = ax)
ax.set_title('Correlation Heatmap')
plt.show()

"""### Check Missing Values"""

# Plotting the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Heatmap of Missing Values')
plt.show()

# Check missing values
print(f"Missing % for Ward: {100*df['Ward'].isnull().sum()/len(df):.2f}%")
print(f"Missing % for Community Area: {100*df['Community Area'].isnull().sum()/len(df):.2f}%")

# Remove correlation between Ward, Community Area
df.drop('Community Area', axis=1, inplace=True) # correlation to Ward

# Remove missing values
df.dropna(subset=['Latitude'], inplace=True) # drop missing values in Latitude/Longitude
df.dropna(subset=['Ward'], inplace=True) # drop missing values in Latitude/Longitude

df.reset_index(drop=True, inplace=True)
df.shape

print(f"Missing % for Ward: {100*df['Ward'].isnull().sum()/len(df):.2f}%")
print(f"Missing % for Latitude: {100*df['Latitude'].isnull().sum()/len(df):.2f}%")
print(f"Missing % for Longitude: {100*df['Longitude'].isnull().sum()/len(df):.2f}%")

df.head()

"""### Create Data Sets with Missing Values (Selected an important feature)"""

def CreateMissing(df, column, percentage):

  # Number of entries to remove
  num_to_remove = int(len(df) * (percentage/100))

  # Randomly choose indices to set to NaN
  np.random.seed(42)  # For reproducibility
  random_indices = np.random.choice(df.index, num_to_remove, replace=False)

  # Introduce missing values in column 'A'
  df.loc[random_indices, column] = np.nan

  return df

df10 = df.copy()
df25 = df.copy()
df50 = df.copy()

df10 = CreateMissing(df10, 'FBI Code', 10)
df25 = CreateMissing(df25, 'FBI Code', 25)
df50 = CreateMissing(df50, 'FBI Code', 50)

print(f"Missing % for FBI Code in df10: {100*df10['FBI Code'].isnull().sum()/len(df):.2f}%")
print(f"Missing % for FBI Code in df25: {100*df25['FBI Code'].isnull().sum()/len(df):.2f}%")
print(f"Missing % for FBI Code in df50: {100*df50['FBI Code'].isnull().sum()/len(df):.2f}%")

"""___
## BASELINE MODELING WITH MEAN IMPUTATION

### BASELINE DATA PREP
"""

var_info, cat_list, num_list, bool_list, dt_list = GetDataInfo(df)
var_info

def DataPrepImputeWithMean(df, use_mean_impute = False):
  # Convert all object columns to categorical
  object_columns = df.select_dtypes(include='object').columns
  df[object_columns] = df[object_columns].apply(lambda x: x.astype('category'))

  # Drop Arrest rows with missing values
  df.dropna(subset=['Arrest'], inplace=True)

  # Define the target variable and features for the sampled data
  y = df['Arrest']
  X = df.drop(['Arrest'], axis=1)

  # Define categorical columns
  categorical_columns = ['Block', 'Description', 'Primary Type',
                        'Location Description', 'time_of_day',
                        'IUCR', 'Beat', 'District', 'Ward','FBI Code']

  # Convert categorical features to dummy variables
  X = pd.get_dummies(X, columns=categorical_columns, drop_first=True)

  # Convert all columns to numeric, coercing errors to NaN
  X = X.apply(pd.to_numeric, errors='coerce')

  # Ensure data types are optimized
  X = X.astype('float32')
  y = y.astype('float32')

  X.replace(to_replace=[None, '', 'NaN'], value=np.nan, inplace=True)
  y.replace(to_replace=[None, '', 'NaN'], value=np.nan, inplace=True)

  # Split the transformed dataset into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # Saving feature names for later use
  feature_list = list(X_train.columns)

  # Impute missing values with the mean value in the training set of the sample
  if use_mean_impute:
    imputer = SimpleImputer(strategy='mean')
    #imputer = IterativeImputer()
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)

  return X_train, X_test, y_train, y_test, feature_list

"""#### Create Data with Simple Imputer = Mean (for 10%, 25%, 50% missing)"""

X_train_simpleimp10, X_test_simpleimp10, y_train_simpleimp10, y_test_simpleimp10, feature_list10 = DataPrepImputeWithMean(df10, use_mean_impute=True)
X_train_simpleimp25, X_test_simpleimp25, y_train_simpleimp25, y_test_simpleimp25, feature_list25 = DataPrepImputeWithMean(df25, use_mean_impute=True)
X_train_simpleimp50, X_test_simpleimp50, y_train_simpleimp50, y_test_simpleimp50, feature_list50 = DataPrepImputeWithMean(df50, use_mean_impute=True)

# check X train size
X_train10_df = pd.DataFrame(X_train_simpleimp10, columns=feature_list10)
X_train10_df.shape

"""### Define Models"""

def run_gbm(X_train, y_train, X_test, y_test):
  gbm = GradientBoostingClassifier()
  gbm.fit(X_train, y_train)

  # Make predictions on the test set
  predictions = gbm.predict(X_test)

  # Evaluate the model's performance using accuracy
  base_gbm_accuracy = accuracy_score(y_test, predictions)
  print("GBM Accuracy:", base_gbm_accuracy)

  return gbm

def run_rf(X_train, y_train, X_test, y_test):
  # Train the model
  rf = RandomForestClassifier()
  rf.fit(X_train, y_train)

  # Make predictions on the test set
  predictions = rf.predict(X_test)

  # Evaluate the model's performance using accuracy
  accuracy = accuracy_score(y_test, predictions)
  print("RF Accuracy:", accuracy)

  return rf

def run_dt(X_train, y_train, X_test, y_test):
  # Train the model
  dt = tree.DecisionTreeClassifier()
  dt.fit(X_train, y_train)

  # Make predictions on the test set
  predictions = dt.predict(X_test)

  # Evaluate the model's performance using accuracy
  accuracy = accuracy_score(y_test, predictions)
  print("Decision Tree Accuracy:", accuracy)

  return dt

def run_ensemble_gbm_rf_dt(X_train, y_train, X_test, y_test):
  dt = tree.DecisionTreeClassifier()
  rf = RandomForestClassifier()
  gbm = GradientBoostingClassifier()

  # Create the Voting classifier with soft voting
  voting_clf = VotingClassifier(estimators=[('gbm', gbm), ('rf', rf), ('dt', dt)], voting='soft')

  # Fit the voting classifier to the training data of the sample
  voting_clf.fit(X_train, y_train)

  # Make predictions with the voting classifier on the test set of the sample
  voting_predictions = voting_clf.predict(X_test)

  # Evaluate the ensemble's performance on the sample
  accuracy = accuracy_score(y_test, voting_predictions)
  print("Voting Ensemble (GBM, RF, DT) Accuracy:", accuracy)

  return voting_clf

def run_ensemble_gbm_rf(X_train, y_train, X_test, y_test):
  dt = tree.DecisionTreeClassifier()
  rf = RandomForestClassifier()
  gbm = GradientBoostingClassifier()

  # Create the Voting classifier with soft voting
  voting_clf = VotingClassifier(estimators=[('gbm', gbm), ('rf', rf)], voting='soft')

  # Fit the voting classifier to the training data of the sample
  voting_clf.fit(X_train, y_train)

  # Make predictions with the voting classifier on the test set of the sample
  voting_predictions = voting_clf.predict(X_test)

  # Evaluate the ensemble's performance on the sample
  accuracy = accuracy_score(y_test, voting_predictions)
  print("Voting Ensemble (GBM, RF) Accuracy:", accuracy)

  return voting_clf

"""
### GBM"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gbmsimple10 = run_gbm(X_train_simpleimp10, y_train_simpleimp10, X_test_simpleimp10, y_test_simpleimp10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gbmsimple25 = run_gbm(X_train_simpleimp25, y_train_simpleimp25, X_test_simpleimp25, y_test_simpleimp25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gbmsimple50 = run_gbm(X_train_simpleimp50, y_train_simpleimp50, X_test_simpleimp50, y_test_simpleimp50)

PlotFeatureImportance(gbmsimple10, feature_list10, 'GBM Varialbe Importances')

"""### Random Forest"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rfsimple10 = run_rf(X_train_simpleimp10, y_train_simpleimp10, X_test_simpleimp10, y_test_simpleimp10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rfsimple25 = run_rf(X_train_simpleimp25, y_train_simpleimp25, X_test_simpleimp25, y_test_simpleimp25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rfsimple50 = run_rf(X_train_simpleimp50, y_train_simpleimp50, X_test_simpleimp50, y_test_simpleimp50)

PlotFeatureImportance(rfsimple10, feature_list10, 'RF Varialbe Importances')

"""### Decision Tree"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dt_simple10 = run_dt(X_train_simpleimp10, y_train_simpleimp10, X_test_simpleimp10, y_test_simpleimp10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dt_simple25 = run_dt(X_train_simpleimp25, y_train_simpleimp25, X_test_simpleimp25, y_test_simpleimp25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dt_simple50 = run_dt(X_train_simpleimp50, y_train_simpleimp50, X_test_simpleimp50, y_test_simpleimp50)

PlotFeatureImportance(dt_simple10, feature_list10, 'Decision Tree Varialbe Importances')

"""### Ensemble (GBM, RF, DT)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensemble1_10 = run_ensemble_gbm_rf_dt(X_train_simpleimp10, y_train_simpleimp10, X_test_simpleimp10, y_test_simpleimp10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensemble1_25 = run_ensemble_gbm_rf_dt(X_train_simpleimp25, y_train_simpleimp25, X_test_simpleimp25, y_test_simpleimp25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensemble1_50 = run_ensemble_gbm_rf_dt(X_train_simpleimp50, y_train_simpleimp50, X_test_simpleimp50, y_test_simpleimp50)

# # Extract feature importances from the RandomForest and GradientBoosting classifiers
# rf_importances = rf.feature_importances_
# gb_importances = gbm.feature_importances_
# dt_importances = dt.feature_importances_

# # Average the feature importances
# # (This can be adjusted based on the contribution or importance of each classifier)
# feature_importances = np.mean([rf_importances, gb_importances, dt_importances], axis=0)

# # Create a DataFrame for better visualization
# importance = pd.DataFrame({
#     'Feature': feature_list,
#     'Importance': feature_importances
# })
# importance = importance.sort_values(by='Importance', ascending=False)[0:100]
# plt.figure(figsize=(18, 6))

# # list of x locations for plotting
# x_values = importance['Feature']

# # Make a bar chart
# plt.bar(importance['Feature'], importance['Importance'], orientation = 'vertical')

# # Tick labels for x axis
# plt.xticks(x_values, importance['Feature'], rotation='vertical')

# # Axis labels and title
# plt.ylabel('Importance'); plt.xlabel('Variable')
# plt.title('Aggregated Feature Importance in Voting Classifier (GBM, RF, DT)');

# importance.to_csv('feature_importance.csv', encoding='utf-8', index=False)

"""### Ensemble (GBM, RF)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensemble2_10 = run_ensemble_gbm_rf(X_train_simpleimp10, y_train_simpleimp10, X_test_simpleimp10, y_test_simpleimp10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensemble2_25 = run_ensemble_gbm_rf(X_train_simpleimp25, y_train_simpleimp25, X_test_simpleimp25, y_test_simpleimp25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensemble2_50 = run_ensemble_gbm_rf(X_train_simpleimp50, y_train_simpleimp50, X_test_simpleimp50, y_test_simpleimp50)

# # Extract feature importances from the RandomForest and GradientBoosting classifiers
# rf_importances = rf.feature_importances_
# gb_importances = gbm.feature_importances_

# # Average the feature importances
# # (This can be adjusted based on the contribution or importance of each classifier)
# feature_importances = np.mean([rf_importances, gb_importances], axis=0)

# # Create a DataFrame for better visualization
# importance = pd.DataFrame({
#     'Feature': feature_list,
#     'Importance': feature_importances
# })
# importance = importance.sort_values(by='Importance', ascending=False)[0:100]
# plt.figure(figsize=(18, 6))

# # list of x locations for plotting
# x_values = importance['Feature']

# # Make a bar chart
# plt.bar(importance['Feature'], importance['Importance'], orientation = 'vertical')

# # Tick labels for x axis
# plt.xticks(x_values, importance['Feature'], rotation='vertical')

# # Axis labels and title
# plt.ylabel('Importance'); plt.xlabel('Variable')
# plt.title('Aggregated Feature Importance in Voting Classifier (GBM, RF)');

"""___
## MODELING FOR IMPUTATION 10% MISSING

### Data Prep for Using a Model as an Imputer for 'FBI Code', 10% Missing
"""

df10_fbi = df10.copy()
df10_fbi.reset_index(drop=True)

# Plotting the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df10_fbi.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Heatmap of 10% Missing FBI Code')
plt.show()

print(f"Missing % for FBI Code: {100*df10_fbi['FBI Code'].isnull().sum()/len(df10_fbi):.2f}%")

"""### IMPUTATION DATA PREP, 10% MISSING"""

# get missing indices
missing10_indices = df10_fbi[df10_fbi["FBI Code"].isna()].index

# Select non-missing indices
non_missing90_indices = df10_fbi.index.difference(missing10_indices)

# Convert all object columns to categorical
object_columns = df10_fbi.select_dtypes(include='object').columns
df10_fbi[object_columns] = df10_fbi[object_columns].apply(lambda x: x.astype('category'))

# Define the target variable and features
y_miss10 = df10_fbi['FBI Code'].cat.codes # encode y
X_miss10 = df10_fbi.drop(['FBI Code'], axis=1)

# Define categorical columns
cat_cols = ['Block', 'IUCR', 'Primary Type', 'Description', 'Location Description',
            'Beat', 'District', 'Ward', 'time_of_day']

# Convert categorical features to dummy variables
X_miss10_onehot = pd.get_dummies(X_miss10, columns=cat_cols, drop_first=False)

# Convert all columns to numeric, coercing errors to NaN
X_miss10_onehot = X_miss10_onehot.apply(pd.to_numeric, errors='coerce')

# Ensure data types are optimized
X_miss10_onehot = X_miss10_onehot.astype('float32')
y_miss10 = y_miss10.astype('float32')

X_90_onehot = X_miss10_onehot.loc[non_missing90_indices]     # X with the 90% no missing FBI Code
X_missing10_onehot = X_miss10_onehot.loc[missing10_indices]  # X with the 10% missing FBI Code
y_90 = y_miss10.loc[non_missing90_indices]                   # y with the 90% no missing FBI Code
y_missing10 = y_miss10.loc[missing10_indices]                # y with the 10% missing FBI Code

# Split the transformed dataset into training and testing sets
X_train_miss10, X_test_miss10, y_train_miss10, y_test_miss10 = train_test_split(X_90_onehot, y_90, test_size=0.2, random_state=42)

"""### RF for 'FBI Code' Imputation, 10% Missing"""

# Train the model using XGBClassifier with enable_categorical=True
rf_fbi = RandomForestClassifier()
rf_fbi.fit(X_train_miss10, y_train_miss10)

# Make predictions on the test set
fbi_predictions = rf_fbi.predict(X_test_miss10)

# Evaluate the model's performance using accuracy
fbi_accuracy = accuracy_score(y_test_miss10, fbi_predictions)
print("FBI Code RF Accuracy:", fbi_accuracy)

# Classification report
# print("Classification Report:")
#print(classification_report(y_test_miss10, fbi_predictions, target_names=df10_fbi['FBI Code'].astype('category').cat.categories))

"""### Impute Missing 'FBI Code', 10% Missing


"""

df10_fbi = df10.copy()
print(f"Missing % for FBI Code: {100*df10_fbi['FBI Code'].isnull().sum()/len(df10_fbi):.2f}%")

# Predict the missing values
imputed_fbi = rf_fbi.predict(X_missing10_onehot)

# Add new categories to the 'FBI Code' column
df10_fbi['FBI Code'] = df10_fbi['FBI Code'].astype('category')
new_categories = pd.Categorical(imputed_fbi).categories
df10_fbi['FBI Code'] = df10_fbi['FBI Code'].cat.add_categories(new_categories)

# Replace missing values in the original dataframe
df10_fbi.loc[missing10_indices, 'FBI Code'] = imputed_fbi

print(f"Missing % for FBI Code: {100*df10_fbi['FBI Code'].isnull().sum()/len(df10_fbi):.2f}%")

X_train10, X_test10, y_train10, y_test10, feature_list10 = DataPrepImputeWithMean(df10_fbi)

"""### CHECK PERFORMANCE, 10% MISSING"""

gbm10 = run_gbm(X_train10, y_train10, X_test10, y_test10)

rf10 = run_rf(X_train10, y_train10, X_test10, y_test10)

dt10 = run_dt(X_train10, y_train10, X_test10, y_test10)

ensemble1_10 = run_ensemble_gbm_rf_dt(X_train10, y_train10, X_test10, y_test10)

ensemble2_10 = run_ensemble_gbm_rf(X_train10, y_train10, X_test10, y_test10)

"""___"""



"""___
## MODELING FOR IMPUTATION 25% MISSING

### Data Prep for Using a Model as an Imputer for 'FBI Code', 10% Missing
"""

df25_fbi = df25.copy()
df25_fbi.reset_index(drop=True)

# Plotting the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df25_fbi.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Heatmap of 25% Missing FBI Code')
plt.show()

print(f"Missing % for FBI Code: {100*df25_fbi['FBI Code'].isnull().sum()/len(df25_fbi):.2f}%")

"""### IMPUTATION DATA PREP, 25% MISSING"""

# get missing indices
missing25_indices = df25_fbi[df25_fbi["FBI Code"].isna()].index

# Select non-missing indices
non_missing75_indices = df25_fbi.index.difference(missing25_indices)

# Convert all object columns to categorical
object_columns = df25_fbi.select_dtypes(include='object').columns
df25_fbi[object_columns] = df25_fbi[object_columns].apply(lambda x: x.astype('category'))

# Define the target variable and features
y_miss25 = df25_fbi['FBI Code'].cat.codes # encode y
X_miss25 = df25_fbi.drop(['FBI Code'], axis=1)

# Define categorical columns
cat_cols = ['Block', 'IUCR', 'Primary Type', 'Description', 'Location Description',
            'Beat', 'District', 'Ward', 'time_of_day']

# Convert categorical features to dummy variables
X_miss25_onehot = pd.get_dummies(X_miss25, columns=cat_cols, drop_first=False)

# Convert all columns to numeric, coercing errors to NaN
X_miss25_onehot = X_miss25_onehot.apply(pd.to_numeric, errors='coerce')

# Ensure data types are optimized
X_miss25_onehot = X_miss25_onehot.astype('float32')
y_miss25 = y_miss25.astype('float32')

X_75_onehot = X_miss25_onehot.loc[non_missing75_indices]     # X with the 75% no missing FBI Code
X_missing25_onehot = X_miss25_onehot.loc[missing25_indices]  # X with the 25% missing FBI Code
y_75 = y_miss25.loc[non_missing75_indices]                   # y with the 75% no missing FBI Code
y_missing25 = y_miss25.loc[missing25_indices]                # y with the 25% missing FBI Code

# Split the transformed dataset into training and testing sets
X_train_miss25, X_test_miss25, y_train_miss25, y_test_miss25 = train_test_split(X_75_onehot, y_75, test_size=0.2, random_state=42)

"""### RF for 'FBI Code' Imputation, 25% Missing"""

# Train the model using XGBClassifier with enable_categorical=True
rf_fbi = RandomForestClassifier()
rf_fbi.fit(X_train_miss25, y_train_miss25)

# Make predictions on the test set
fbi_predictions = rf_fbi.predict(X_test_miss25)

# Evaluate the model's performance using accuracy
fbi_accuracy = accuracy_score(y_test_miss25, fbi_predictions)
print("FBI Code RF Accuracy:", fbi_accuracy)

# Classification report
print("Classification Report:")
#print(classification_report(y_test_miss25, fbi_predictions, target_names=df25_fbi['FBI Code'].astype('category').cat.categories))

"""### Impute Missing 'FBI Code', 25% Missing


"""

df25_fbi = df25.copy()
print(f"Missing % for FBI Code: {100*df25_fbi['FBI Code'].isnull().sum()/len(df25_fbi):.2f}%")

# Predict the missing values
imputed_fbi = rf_fbi.predict(X_missing25_onehot)

# Add new categories to the 'FBI Code' column
df25_fbi['FBI Code'] = df25_fbi['FBI Code'].astype('category')
new_categories = pd.Categorical(imputed_fbi).categories
df25_fbi['FBI Code'] = df25_fbi['FBI Code'].cat.add_categories(new_categories)

# Replace missing values in the original dataframe
df25_fbi.loc[missing25_indices, 'FBI Code'] = imputed_fbi

print(f"Missing % for FBI Code: {100*df25_fbi['FBI Code'].isnull().sum()/len(df25_fbi):.2f}%")

X_train25, X_test25, y_train25, y_test25, feature_list25 = DataPrepImputeWithMean(df25_fbi)

"""### CHECK PERFORMANCE, 25% MISSING"""

rf25 = run_rf(X_train25, y_train25, X_train25, y_train25)

gbm25 = run_gbm(X_train25, y_train25, X_test25, y_test25)

rf25 = run_rf(X_train25, y_train25, X_test25, y_test25)

dt25 = run_dt(X_train25, y_train25, X_test25, y_test25)

ensemble1_25 = run_ensemble_gbm_rf_dt(X_train25, y_train25, X_test25, y_test25)

ensemble2_25 = run_ensemble_gbm_rf(X_train25, y_train25, X_test25, y_test25)

"""___
## MODELING FOR IMPUTATION 50% MISSING

### Data Prep for Using a Model as an Imputer for 'FBI Code', 50% Missing
"""

df50_fbi = df50.copy()
df50_fbi.reset_index(drop=True)

# Plotting the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df50_fbi.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Heatmap of 50% Missing FBI Code')
plt.show()

print(f"Missing % for FBI Code: {100*df50_fbi['FBI Code'].isnull().sum()/len(df50_fbi):.2f}%")

"""### IMPUTATION DATA PREP, 50% MISSING"""

# get missing indices
missing50_indices = df50_fbi[df50_fbi["FBI Code"].isna()].index

# Select non-missing indices
non_missing50_indices = df50_fbi.index.difference(missing50_indices)

# Convert all object columns to categorical
object_columns = df50_fbi.select_dtypes(include='object').columns
df50_fbi[object_columns] = df50_fbi[object_columns].apply(lambda x: x.astype('category'))

# Define the target variable and features
y_miss50 = df50_fbi['FBI Code'].cat.codes # encode y
X_miss50 = df50_fbi.drop(['FBI Code'], axis=1)

# Define categorical columns
cat_cols = ['Block', 'IUCR', 'Primary Type', 'Description', 'Location Description',
            'Beat', 'District', 'Ward', 'time_of_day']

# Convert categorical features to dummy variables
X_miss50_onehot = pd.get_dummies(X_miss50, columns=cat_cols, drop_first=False)

# Convert all columns to numeric, coercing errors to NaN
X_miss50_onehot = X_miss50_onehot.apply(pd.to_numeric, errors='coerce')

# Ensure data types are optimized
X_miss50_onehot = X_miss50_onehot.astype('float32')
y_miss50 = y_miss50.astype('float32')

X_50_onehot = X_miss50_onehot.loc[non_missing50_indices]     # X with the 50% no missing FBI Code
X_missing50_onehot = X_miss50_onehot.loc[missing50_indices]  # X with the 50% missing FBI Code
y_50 = y_miss50.loc[non_missing50_indices]                   # y with the 50% no missing FBI Code
y_missing50 = y_miss50.loc[missing50_indices]                # y with the 50% missing FBI Code

# Split the transformed dataset into training and testing sets
X_train_miss50, X_test_miss50, y_train_miss50, y_test_miss50 = train_test_split(X_50_onehot, y_50, test_size=0.2, random_state=42)

"""### RF for 'FBI Code' Imputation, 50% Missing"""

# Train the model using XGBClassifier with enable_categorical=True
rf_fbi = RandomForestClassifier()
rf_fbi.fit(X_train_miss50, y_train_miss50)

# Make predictions on the test set
fbi_predictions = rf_fbi.predict(X_test_miss50)

# Evaluate the model's performance using accuracy
fbi_accuracy = accuracy_score(y_test_miss50, fbi_predictions)
print("FBI Code RF Accuracy:", fbi_accuracy)

# Classification report
print("Classification Report:")
#print(classification_report(y_test_miss25, fbi_predictions, target_names=df25_fbi['FBI Code'].astype('category').cat.categories))

"""### Impute Missing 'FBI Code', 50% Missing


"""

df50_fbi = df50.copy()
print(f"Missing % for FBI Code: {100*df50_fbi['FBI Code'].isnull().sum()/len(df50_fbi):.2f}%")

# Predict the missing values
imputed_fbi = rf_fbi.predict(X_missing50_onehot)

# Add new categories to the 'FBI Code' column
df50_fbi['FBI Code'] = df50_fbi['FBI Code'].astype('category')
new_categories = pd.Categorical(imputed_fbi).categories
df50_fbi['FBI Code'] = df50_fbi['FBI Code'].cat.add_categories(new_categories)

# Replace missing values in the original dataframe
df50_fbi.loc[missing50_indices, 'FBI Code'] = imputed_fbi

print(f"Missing % for FBI Code: {100*df50_fbi['FBI Code'].isnull().sum()/len(df50_fbi):.2f}%")

X_train50, X_test50, y_train50, y_test50, feature_list50 = DataPrepImputeWithMean(df50_fbi)

"""### CHECK PERFORMANCE RF, 50% MISSING"""

gbm50 = run_gbm(X_train50, y_train50, X_test50, y_test50)

rf50 = run_rf(X_train50, y_train50, X_test50, y_test50)

dt50 = run_dt(X_train50, y_train50, X_test50, y_test50)

ensemble2_50 = run_ensemble_gbm_rf_dt(X_train50, y_train50, X_test50, y_test50)

ensemble2_50 = run_ensemble_gbm_rf(X_train50, y_train50, X_test50, y_test50)



